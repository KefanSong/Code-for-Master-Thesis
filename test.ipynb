{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0036, -0.0063, -0.0076,  0.0031, -0.0078]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([[-0.5000, -0.5000, -0.5000, -0.5000, -0.5000]],\n",
      "       grad_fn=<ExpandBackward0>) tensor([[0.6065, 0.6065, 0.6065, 0.6065, 0.6065]], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class SharedNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_dim\n",
    "        for size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(activation())\n",
    "            prev_size = size\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=(64, 64), activation=nn.Tanh, log_std=-0.5):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.mlp_net = SharedNet(obs_dim, hidden_sizes, activation)\n",
    "        self.mean_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.logstd_layer = nn.Parameter(torch.ones(1, act_dim) * log_std)\n",
    "        self.mean_layer.weight.data.mul_(0.1)\n",
    "        self.mean_layer.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        out = self.mlp_net(obs)\n",
    "        mean = self.mean_layer(out)\n",
    "        if len(mean.size()) == 1:\n",
    "            mean = mean.view(1, -1)\n",
    "        logstd = self.logstd_layer.expand_as(mean)\n",
    "        std = torch.exp(logstd)\n",
    "        return mean, logstd, std\n",
    "\n",
    "    def get_act(self, obs, deterministic=False):\n",
    "        mean, _, std = self.forward(obs)\n",
    "        if deterministic:\n",
    "            return mean\n",
    "        else:\n",
    "            return torch.normal(mean, std)\n",
    "\n",
    "    def logprob(self, obs, act):\n",
    "        mean, _, std = self.forward(obs)\n",
    "        normal = Normal(mean, std)\n",
    "        return normal.log_prob(act).sum(-1, keepdim=True), mean, std\n",
    "\n",
    "#\n",
    "class MultiGaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dims, shared_hidden_sizes=(64, 64), task_hidden_sizes=(64, 64), activation=nn.Tanh, log_std=-0.5):\n",
    "        super().__init__()\n",
    "        self.shared_net = SharedNet(obs_dim, shared_hidden_sizes, activation)\n",
    "        self.task_policies = nn.ModuleList([\n",
    "            GaussianPolicy(shared_hidden_sizes[-1], act_dim, task_hidden_sizes, activation, log_std) for act_dim in act_dims\n",
    "            ])\n",
    "        #[ task1_GaussianPolicy,task2_GaussianPolicy,....., ]\n",
    "\n",
    "    def forward(self, obs, task_idx):\n",
    "        shared_out = self.shared_net(obs)\n",
    "        return self.task_policies[task_idx](shared_out)\n",
    "    def logprob(self, obs, act,task_idx):\n",
    "        mean, _, std = self.forward(obs,task_idx)\n",
    "        normal = Normal(mean, std)\n",
    "        return normal.log_prob(act).sum(-1, keepdim=True), mean, std\n",
    "\n",
    "# Example usage\n",
    "obs_dim = 10\n",
    "act_dims = [5, 3]  # Two tasks with different action dimensions\n",
    "policy = MultiGaussianPolicy(obs_dim, act_dims)\n",
    "\n",
    "obs = torch.randn(1, obs_dim)\n",
    "task_idx = 0  # Select the first task\n",
    "mean, logstd, std = policy(obs, task_idx)\n",
    "print(mean, logstd, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "def mlp(input_size, hidden_sizes=(64, 64), activation='tanh'):\n",
    "\n",
    "    if activation == 'tanh':\n",
    "        activation = nn.Tanh\n",
    "    elif activation == 'relu':\n",
    "        activation = nn.ReLU\n",
    "    elif activation == 'sigmoid':\n",
    "        activation = nn.Sigmoid\n",
    "\n",
    "    layers = []\n",
    "    sizes = (input_size, ) + hidden_sizes\n",
    "    for i in range(len(hidden_sizes)):\n",
    "        layers += [nn.Linear(sizes[i], sizes[i+1]), activation()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "# class GaussianPolicy(nn.Module):\n",
    "#     def __init__(self, obs_dim, act_dim, hidden_sizes=(64, 64), activation='tanh', log_std=-0.5):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.obs_dim = obs_dim\n",
    "#         self.act_dim = act_dim\n",
    "\n",
    "#         self.mlp_net = mlp(obs_dim, hidden_sizes, activation)\n",
    "#         self.mean_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "\n",
    "#         self.logstd_layer = nn.Parameter(torch.ones(1, act_dim) * log_std)\n",
    "\n",
    "#         self.mean_layer.weight.data.mul_(0.1)\n",
    "#         self.mean_layer.bias.data.mul_(0.0)\n",
    "        \n",
    "#         # for a second head for the second task\n",
    "#         self.mean_layer2 = nn.Linear(hidden_sizes[-1], act_dim) \n",
    "#         self.logstd_layer2 = nn.Parameter(torch.ones(1, act_dim) * log_std)\n",
    "\n",
    "#         self.mean_layer2.weight.data.mul_(0.1)\n",
    "#         self.mean_layer2.bias.data.mul_(0.0)\n",
    "\n",
    "#     def forward(self, obs, task=1):\n",
    "\n",
    "#         out = self.mlp_net(obs)\n",
    "\n",
    "        \n",
    "#         if task==1:\n",
    "            \n",
    "#             mean = self.mean_layer(out)\n",
    "#             if len(mean.size()) == 1:\n",
    "#                 mean = mean.view(1, -1)\n",
    "#             logstd = self.logstd_layer.expand_as(mean)\n",
    "#             std = torch.exp(logstd)\n",
    "#         else:\n",
    "            \n",
    "#             mean = self.mean_layer2(out)\n",
    "#             if len(mean.size()) == 1:\n",
    "#                 mean = mean.view(1, -1)\n",
    "#             logstd = self.logstd_layer2.expand_as(mean)\n",
    "#             std = torch.exp(logstd)\n",
    "            \n",
    "#         return mean, logstd, std\n",
    "\n",
    "#     def get_act(self, obs, deterministic = False, task=1):\n",
    "#         mean, _, std = self.forward(obs, task)\n",
    "#         if deterministic:\n",
    "#             return mean\n",
    "#         else:\n",
    "#             return torch.normal(mean, std)\n",
    "\n",
    "#     def logprob(self, obs, act, task=1):\n",
    "#         mean, _, std = self.forward(obs)\n",
    "#         normal = Normal(mean, std)\n",
    "#         return normal.log_prob(act).sum(-1, keepdim=True), mean, std\n",
    "\n",
    "\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_sizes=(64, 64), activation='tanh'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "\n",
    "        self.mlp_net = mlp(obs_dim, hidden_sizes, activation)\n",
    "        self.v_head = nn.Linear(hidden_sizes[-1], 1)\n",
    "\n",
    "        self.v_head.weight.data.mul_(0.1)\n",
    "        self.v_head.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mlp_out = self.mlp_net(obs)\n",
    "        v_out = self.v_head(mlp_out)\n",
    "        return v_out\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class SharedNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_dim\n",
    "        for size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(activation())\n",
    "            prev_size = size\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=(64, 64), activation=nn.Tanh, log_std=-0.5):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.mlp_net = SharedNet(obs_dim, hidden_sizes, activation)\n",
    "        self.mean_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.logstd_layer = nn.Parameter(torch.ones(1, act_dim) * log_std)\n",
    "        self.mean_layer.weight.data.mul_(0.1)\n",
    "        self.mean_layer.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        out = self.mlp_net(obs)\n",
    "        mean = self.mean_layer(out)\n",
    "        if len(mean.size()) == 1:\n",
    "            mean = mean.view(1, -1)\n",
    "        logstd = self.logstd_layer.expand_as(mean)\n",
    "        std = torch.exp(logstd)\n",
    "        return mean, logstd, std\n",
    "\n",
    "    def get_act(self, obs, deterministic=False):\n",
    "        mean, _, std = self.forward(obs)\n",
    "        if deterministic:\n",
    "            return mean\n",
    "        else:\n",
    "            return torch.normal(mean, std)\n",
    "\n",
    "    def logprob(self, obs, act):\n",
    "        mean, _, std = self.forward(obs)\n",
    "        normal = Normal(mean, std)\n",
    "        return normal.log_prob(act).sum(-1, keepdim=True), mean, std\n",
    "\n",
    "#\n",
    "class MultiGaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dims, shared_hidden_sizes=(64, 64), task_hidden_sizes=(64, 64), activation=nn.Tanh, log_std=-0.5):\n",
    "        super().__init__()\n",
    "        self.shared_net = SharedNet(obs_dim, shared_hidden_sizes, activation)\n",
    "        self.task_policies = nn.ModuleList([\n",
    "            GaussianPolicy(shared_hidden_sizes[-1], act_dim, task_hidden_sizes, activation, log_std) for act_dim in act_dims\n",
    "            ])\n",
    "        #[ task1_GaussianPolicy,task2_GaussianPolicy,....., ]\n",
    "\n",
    "    def forward(self, obs, task_idx):\n",
    "        shared_out = self.shared_net(obs)\n",
    "        return self.task_policies[task_idx](shared_out)\n",
    "    def logprob(self, obs, act,task_idx):\n",
    "        mean, _, std = self.forward(obs,task_idx)\n",
    "        normal = Normal(mean, std)\n",
    "        return normal.log_prob(act).sum(-1, keepdim=True), mean, std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m obs_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      2\u001b[0m act_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m----> 3\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mMultiGaussianPolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mact_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_dim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTanh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 155\u001b[0m, in \u001b[0;36mMultiGaussianPolicy.__init__\u001b[0;34m(self, obs_dim, act_dims, shared_hidden_sizes, task_hidden_sizes, activation, log_std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs_dim, act_dims, shared_hidden_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m), task_hidden_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m), activation\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mTanh, log_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_net \u001b[38;5;241m=\u001b[39m \u001b[43mSharedNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_hidden_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_policies \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    157\u001b[0m         GaussianPolicy(shared_hidden_sizes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], act_dim, task_hidden_sizes, activation, log_std) \u001b[38;5;28;01mfor\u001b[39;00m act_dim \u001b[38;5;129;01min\u001b[39;00m act_dims\n\u001b[1;32m    158\u001b[0m         ])\n",
      "Cell \u001b[0;32mIn[2], line 111\u001b[0m, in \u001b[0;36mSharedNet.__init__\u001b[0;34m(self, input_dim, hidden_sizes, activation)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m hidden_sizes:\n\u001b[1;32m    110\u001b[0m     layers\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mLinear(prev_size, size))\n\u001b[0;32m--> 111\u001b[0m     layers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    112\u001b[0m     prev_size \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39mlayers)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "obs_dim = 64\n",
    "act_dim = 16\n",
    "policy = MultiGaussianPolicy(obs_dim, [act_dim, act_dim], (64, 64), nn.Tanh, -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
